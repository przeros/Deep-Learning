{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsfXd4V0yocP"
      },
      "source": [
        "##### AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Programu Operacyjnego Polska Cyfrowa na lata 2014-2020\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "YmsuYAvAzBaa",
        "outputId": "c59700ea-b366-4028-b6f0-88cacd73aee4"
      },
      "source": [
        "#@title\n",
        "%%html\n",
        "<iframe src=\"https://www.polskacyfrowa.gov.pl/media/48246/FE_POPC_poziom_pl-1_rgb.jpg\" width=\"800\"></iframe>\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe src=\"https://www.polskacyfrowa.gov.pl/media/48246/FE_POPC_poziom_pl-1_rgb.jpg\" width=\"800\"></iframe>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdecyDDYyzRr"
      },
      "source": [
        "# Uczenie głębokie\n",
        "\n",
        "Szymon Zaporowski, Politechnika Gdańska, Wydział ETI, Katedra Systemów Multimedialnych\n",
        "\n",
        "**Wykład 7:** Rekurencyjne Sieci Neuronowe\n",
        "\n",
        "**Przykład (1):** Wprowadzenie do Rekurencyjnych Sieci Neuronowych\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVq5mz9s8CPL"
      },
      "source": [
        "## Tworzenie sieci RNN z wykorzystaniem pakietu PyTorch\n",
        "\n",
        "W tym notaniku pokazano prosty sposób budowy sieci RNN od zera z wykorzystaniem pakietu PyTorch. W ramach notatnika zostanie stworzona siec składajaca sie z jednej warstwy. Taka prosta konstrukcja pozwoli na prześledzenie jak właściwie zbudowana jest komórka sieci RNN i jak wygląda inicjowanie wag oraz stanów ukrytych.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXs_oWct-qxY"
      },
      "source": [
        "\n",
        "Wskażmy pakiety, z jakich będziemy korzystać:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBuAr4-L8CPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd7cba5-f2e6-412c-dc7e-1185359a84c8"
      },
      "source": [
        "!pip3 install torch torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMQfRrRl8CPV"
      },
      "source": [
        "### Sieć RNN z pojedynczym neuronem\n",
        "\n",
        "\n",
        "Zacznijmy od prostego przykładu, aby zrozumieć mechanizm stojący za działaniem sieci typu RNN.\n",
        "Na samym początku stwórzmy graf obliczeniowy dla jednowarstwowej sieci RNN.\n",
        "\n",
        "Poniżej przedstawiono schemat architektury, którą budujemy:\n",
        "![](https://drive.google.com/uc?export=view&id=127yxax8AuwdBOsWP-Wg-OsZaD4nw2abW)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHjE4j2x8CPW"
      },
      "source": [
        "Poniżej znajduje się kod z implementacją przedstawionej na powyższym rysunku sieci. Wykonajmy komórkę i prześledźmy co własciwie dzieje się w kodzie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiY3EllI8CPY"
      },
      "source": [
        "class  BasicRNN(nn.Module):\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        super(BasicRNN, self).__init__()\n",
        "\n",
        "        n_neurons=n_neurons\n",
        "\n",
        "        self.Wx = torch.randn(n_inputs, n_neurons) # 4 X n_neurons\n",
        "        self.Wy = torch.randn(n_neurons, n_neurons) # 1 X n_neurons\n",
        "\n",
        "        self.b = torch.zeros(1, n_neurons) # 1 X 4\n",
        "\n",
        "    def forward(self, X0, X1):\n",
        "        self.Y0 = torch.tanh(torch.mm(X0, self.Wx) + self.b) # 4 X n_neurons\n",
        "\n",
        "        self.Y1 = torch.tanh(torch.mm(self.Y0, self.Wy) +\n",
        "                            torch.mm(X1, self.Wx) + self.b) # 4 X n_neurons\n",
        "\n",
        "        return self.Y0, self.Y1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP94VBHE8CPc"
      },
      "source": [
        "W powyższek komórce zaimplementowano najprostszą sieć RNN składającą się z jednej warstwy i jednego neurona. Zostały zainicjalizowane dwie macierze wag - `Wx` oraz `Wy` posiadające wartości pochodzące z rozkładu normalnego. `Wx`  zawiera wagi połączeń dla wejścia w obecnym kroku czasowym, podczas gdy `Wy` zawiera wagi połączeń dla wyjść z poprzedniego kroku czasowego. Dodatkowo zastosowano bias ukryty pod zmienną `b` . Funkcja `forward` oblicza dwa wyjścia - po jednym dla każdego kroku czasowego. Zastosowano funkcję tangensa hiperbolicznego `tanh` jako funkcję aktywacji.\n",
        "\n",
        "Jako wejście wprowadzane są 4 przykłady, gdzie każdy przykład zawiera dwie sekwencje wejściowe.\n",
        "\n",
        "Poniżej schematycznie przedstawiono w jaki sposób wprowadzane są dane do modelu sieci RNN:\n",
        "![](https://drive.google.com/uc?export=view&id=18VUYxUyXlo2jbqhHNjfcv8igX1hU38WT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydSxau_eFpwH"
      },
      "source": [
        "Wykonajmy poniższą komórkę, aby przetestować model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a7-kIhj8CPe"
      },
      "source": [
        "N_INPUT = 4\n",
        "N_NEURONS = 1\n",
        "\n",
        "X0_batch = torch.tensor([[0,1,2,0], [3,4,5,0],\n",
        "                         [6,7,8,0], [9,0,1,0]],\n",
        "                        dtype = torch.float) #t=0 => 4 X 4\n",
        "\n",
        "X1_batch = torch.tensor([[9,8,7,0], [0,0,0,0],\n",
        "                         [6,5,4,0], [3,2,1,0]],\n",
        "                        dtype = torch.float) #t=1 => 4 X 4\n",
        "model = BasicRNN(N_INPUT, N_NEURONS)\n",
        "\n",
        "Y0_val, Y1_val = model(X0_batch, X1_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNaFkJUP8CPj"
      },
      "source": [
        "Po podaniu na wejście grafu obliczeniowego wartości uzyskaliśmy wartości wyjść dla kroków czasowych `Y0` oraz `Y1`. Zobaczmy jak wyglądają wartości tych wyjść.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-teLNoV8CPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f16c2cb-c869-4959-8328-09a3ba8d029e"
      },
      "source": [
        "print(Y0_val)\n",
        "print(Y1_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6834],\n",
            "        [-0.9635],\n",
            "        [-0.9963],\n",
            "        [-0.9984]])\n",
            "tensor([[-0.9990],\n",
            "        [-0.9312],\n",
            "        [-0.9967],\n",
            "        [-0.9673]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-k7iJrs3RA1"
      },
      "source": [
        "Wyjścia przyjmują wartość wektora o rozmiarze 4x1, zgodnie ze zdefiniowaną funkcją `forward`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6tcX6ii8CP0"
      },
      "source": [
        "### Zwiększanie liczby neuronów w warstwie sieci RNN\n",
        "Teraz czas na zwiększenie libczy nueronów w warstwie, tak aby pojedyncza warstwa mogła posiadać liczbę neuronów równą `n` . Należy pamiętać, że w odniesieniu do architektury nie dokonujemy żadnych zmian, gdyż liczba neuronów została już sparametryzowana w grafie obliczeniowym. Jednak zmianabędzie dotyczyła rozmiaru wyjścia, ponieważ został zmieniony rozmiar jednostek (czyli neuronów) w warstwie RNN.\n",
        "\n",
        "Ponizej ilustracja jak będzie wyglądała taka architektrura:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=16VQBp3paGSiUgRE6LVUnLoxV8FsTwI4d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5e2Eh5dGvnB"
      },
      "source": [
        "Poniżej przedstawiono kod jak stworzyć architekturę zgodną z powyższą ilustracją, skorzytsamy z wcześniejszej klasy `BasicRNN`, zmienimy liczbę cech wejśćia i neuronów oraz wartość wejść:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlc4vEf88CP5"
      },
      "source": [
        "N_INPUT = 3 #  liczba cech w wejściu\n",
        "N_NEURONS = 4 # liczba jednostek (neuronów) w warstwie\n",
        "X0_batch = torch.tensor([[0,1,2], [3,4,5],\n",
        "                         [6,7,8], [9,0,1]],\n",
        "                        dtype = torch.float) # krok czasowy t=0 => rozmiar 4 X 3\n",
        "\n",
        "X1_batch = torch.tensor([[9,8,7], [0,0,0],\n",
        "                         [6,5,4], [3,2,1]],\n",
        "                        dtype = torch.float) # krok czasowy t=1 => rozmiar 4 X 3\n",
        "\n",
        "model = BasicRNN(N_INPUT, N_NEURONS)\n",
        "\n",
        "Y0_val, Y1_val = model(X0_batch, X1_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9VNN7jf8CP9"
      },
      "source": [
        "Teraz, gdy zostaną wyświetlone wartości wyjścia dla każdego kroku czasowego przyjmą one rozmiar `4 X 5` co jest związane odpowiednio z rozmiarem batcha oraz liczbą neuronów\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1kiEzop8CP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01dae399-96d6-4046-bcce-f9a87fd432d0"
      },
      "source": [
        "print(Y0_val)\n",
        "print(Y1_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.9700,  0.0453,  0.9774,  0.9987],\n",
            "        [ 0.9990,  0.5781,  1.0000,  1.0000],\n",
            "        [ 1.0000,  0.8549,  1.0000,  1.0000],\n",
            "        [-0.8588, -0.9922,  0.9984, -1.0000]])\n",
            "tensor([[ 0.9995,  0.6149,  1.0000,  0.9901],\n",
            "        [ 0.5938, -0.6128,  0.4187, -0.2809],\n",
            "        [ 0.9504,  0.5702,  1.0000, -0.4382],\n",
            "        [-0.5133,  0.9162,  0.9921, -0.9553]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRKjsv2t8CQG"
      },
      "source": [
        "### Bardziej skomplikowana architektura RNN z wykorzystaniem RNNCell z pakiety PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBVgdr9t7guz"
      },
      "source": [
        "Zapewne patrząc na wcześniejszy kod można było sobie zadać pewne pytanie. W trakcie wykładu była mowa o tym, że RNN pozwalają na korzystanie z wartości wejść i wyjść, które mają znaczne wymiary. Gdyby budować sieć w pokazany powyżej sposób konieczne byłoby pojedyncze obliczanie wyjścia dla każdego kroku czasowego, tym samym powdoując znaczny przyrost liczby linijek kodu, aby uzyskać zadowalający efekt w grafie obliczeniowym. Poniżej pokazano sposób w jaki można zaimplementować takie podejście z wykorzystaniem modułu RNNCell.\n",
        "\n",
        "Przeanalizujmy strukturę modułu `RNNCell` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp0Wjh4Z8CQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4533b715-8864-407e-fd44-9fc32ecc5826"
      },
      "source": [
        "rnn = nn.RNNCell(3, 5) # liczba_wejść X liczba_neuronów\n",
        "\n",
        "X_batch = torch.tensor([[[0,1,2], [3,4,5],\n",
        "                         [6,7,8], [9,0,1]],\n",
        "                        [[9,8,7], [0,0,0],\n",
        "                         [6,5,4], [3,2,1]]\n",
        "                       ], dtype = torch.float) # X0 and X1\n",
        "\n",
        "hx = torch.randn(4, 5) # m X liczba_neuronów -> poprzedni krok czasowy\n",
        "output = []\n",
        "\n",
        "# dla każdego kroku czasowego\n",
        "for i in range(2):\n",
        "    hx = rnn(X_batch[i], hx)\n",
        "    output.append(hx)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 0.1770,  0.7625, -0.4189,  0.0832, -0.3649],\n",
            "        [-0.8115,  0.9066, -0.4927,  0.9082, -0.9699],\n",
            "        [-0.8859,  0.9730, -0.5052,  0.9994, -0.9981],\n",
            "        [-0.9996, -0.6566, -0.9573,  0.2146, -0.9843]],\n",
            "       grad_fn=<TanhBackward0>), tensor([[-0.9899,  0.8744, -0.9456,  0.9990, -0.9994],\n",
            "        [ 0.1901, -0.2070, -0.5764, -0.5149,  0.4377],\n",
            "        [-0.9605,  0.4545, -0.9505,  0.9586, -0.9760],\n",
            "        [-0.8177,  0.6691, -0.7424,  0.2129, -0.8660]],\n",
            "       grad_fn=<TanhBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRNUCsXl8CQN"
      },
      "source": [
        "W powyższej komórce udało się zaimplementować bardzo podobne rozwiązanie jak `BasicRNN` pokazane w 7 komórce. Jednak wykorzystanie `torch.RNNCell(...)` uwalnia nas z konieczności deklarowania macierzy wag i biasów - jest to wykonywane automatycznie za nas. `torch.RNNCell`  akcpetyje tensor jako wejście i wyjscie następnego stanu ukrytego `hx` dla każdego elementu w batchu.\n",
        "\n",
        "Stwórzmy teraz graf obliczeniowy wykorzstując wiedzę z poprzedniej komórki:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Opob45Zj8CQP"
      },
      "source": [
        "class CleanBasicRNN(nn.Module):\n",
        "    def __init__(self, batch_size, n_inputs, n_neurons):\n",
        "        super(CleanBasicRNN, self).__init__()\n",
        "\n",
        "        self.rnn = nn.RNNCell(n_inputs, n_neurons)\n",
        "        self.hx = torch.randn(batch_size, n_neurons) # inicjalizacja ukrytego stanu\n",
        "\n",
        "    def forward(self, X):\n",
        "        output = []\n",
        "\n",
        "        # dla każdego kroku czasowego\n",
        "        for i in range(2):\n",
        "            self.hx = self.rnn(X[i], self.hx)\n",
        "            output.append(self.hx)\n",
        "\n",
        "        return output, self.hx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL1yBNis8CQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a9c642c-0a9e-480e-99c2-a68cd60832a9"
      },
      "source": [
        "FIXED_BATCH_SIZE = 4 # batch size jest wartością stałą\n",
        "N_INPUT = 3\n",
        "N_NEURONS = 5\n",
        "\n",
        "X_batch = torch.tensor([[[0,1,2], [3,4,5],\n",
        "                         [6,7,8], [9,0,1]],\n",
        "                        [[9,8,7], [0,0,0],\n",
        "                         [6,5,4], [3,2,1]]\n",
        "                       ], dtype = torch.float) # X0 and X1\n",
        "\n",
        "\n",
        "model = CleanBasicRNN(FIXED_BATCH_SIZE, N_INPUT, N_NEURONS)\n",
        "output_val, states_val = model(X_batch)\n",
        "print('Wartość wyjść:',output_val) # zawiera wszystkie wartości wyjścia dla wszystkich obliczonych kroków czasowych\n",
        "print(\"Wartość stanu:\",states_val) #zawiera wartość dla finalnego stanu lub kroku czasowego np. t=1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wartość wyjść: [tensor([[ 0.2094,  0.4826,  0.6720,  0.3811,  0.4934],\n",
            "        [-0.9826,  0.8380, -0.6931,  0.9694,  0.9147],\n",
            "        [-0.9998,  0.9849, -0.9526,  0.9991,  0.9972],\n",
            "        [-0.3999,  0.8226, -0.5932,  0.9996,  0.9992]],\n",
            "       grad_fn=<TanhBackward0>), tensor([[-1.0000,  0.9915, -0.9990,  1.0000,  0.9998],\n",
            "        [ 0.0246,  0.1705,  0.2521, -0.5257,  0.5569],\n",
            "        [-0.9983,  0.9102, -0.9892,  0.9960,  0.9981],\n",
            "        [-0.8526,  0.4656, -0.7301,  0.8870,  0.9810]],\n",
            "       grad_fn=<TanhBackward0>)]\n",
            "Wartość stanu: tensor([[-1.0000,  0.9915, -0.9990,  1.0000,  0.9998],\n",
            "        [ 0.0246,  0.1705,  0.2521, -0.5257,  0.5569],\n",
            "        [-0.9983,  0.9102, -0.9892,  0.9960,  0.9981],\n",
            "        [-0.8526,  0.4656, -0.7301,  0.8870,  0.9810]],\n",
            "       grad_fn=<TanhBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l240cp4g8CQh"
      },
      "source": [
        "Jak widać kod jest prostszy, nie musimy ręcznie aktualizować wartości macierzy wag - wszystko jest wykonywane automatycznie przez PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIinhhaX-9zp"
      },
      "source": [
        "Zachęcam do eksperymentowania z notatnikiem, prób zwiększania liczby warstw i liczby neuronów, funkcji aktywacji itp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqiHTAud-8-X"
      },
      "source": [
        "<center>\n",
        "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego\n",
        "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
        "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\"\n",
        "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
        "    </center>"
      ]
    }
  ]
}